<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>tmpt6rs5kmw</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 100%;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<h1 id="introduction">Introduction</h1>
<p>Financial mathematics is often obscured by a forest of indices and
sums. By adopting the language of differential geometry, we reveal the
simple structures underneath. This note establishes two geometric
settings:</p>
<ol type="1">
<li><p><strong>The Manifold of Assets</strong>: Where portfolios live.
Here, risk defines distances and angles.</p></li>
<li><p><strong>The Manifold of Statistics</strong>: Where estimators
live. Here, the Central Limit Theorem defines the local tangent
structure.</p></li>
</ol>
<p><strong>Why Geometry?</strong> This approach offers three distinct
advantages:</p>
<ol type="1">
<li><p><strong>Unified Intuition</strong>: Both domains are governed by
a <em>metric tensor</em> representing "cost". In asset space, Œ£ measures
the cost of risk; in statistical space, Œì measures the cost of
uncertainty.</p></li>
<li><p><strong>Coordinate Invariance</strong>: Financial quantities
(Sharpe, PnL) are scalars and must not depend on arbitrary choices of
basis (e.g., dollars vs.¬†cents, central vs.¬†raw moments). Geometry
enforces this invariance by construction.</p></li>
<li><p><strong>Computational Power</strong>: Complex derivations, such
as the asymptotic variance of the Sharpe ratio, reduce from pages of
algebra to simple tensor contractions.</p></li>
</ol>
<h1 id="part-i-the-riemannian-geometry-of-assets">Part I: The Riemannian
Geometry of Assets</h1>
<p>Consider the space of N assets. A portfolio is specified by its
allocation vector w.</p>
<h2 id="vectors-duals-and-the-metric">Vectors, Duals, and the
Metric</h2>
<p>We must rigorously distinguish between two types of objects in the
coordinate basis:</p>
<ul>
<li><p><strong>Allocations are Contravariant Vectors (w‚Å±)</strong>: They
represent the state of the system (quantities held, e.g., number of
shares). They have Volume units [V]. Under a change of basis (e.g.,
stock split S' = Œª S), the components transform inversely to the basis:
w' = (1/Œª) w.</p></li>
<li><p><strong>Alphas are Covariant Covectors (Œ±·µ¢)</strong>: They
represent the gradient of expected return (force acting on the system).
They have units of Pressure [P] so that the product Œ±·µ¢ w‚Å± is money (or
unitless return). Under a change of basis, they transform covariantly
with the basis: Œ±' = Œª Œ±.</p></li>
</ul>
<p>The expected PnL is the contraction ùîº[PnL] = Œ±·µ¢ w‚Å±. To discuss risk,
we introduce the covariance metric g·µ¢‚±º = Œ£·µ¢‚±º.<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<h2 id="the-risk-frame-vielbein">The Risk Frame (Vielbein)</h2>
<p>The metric g·µ¢‚±º is generally not identity (assets are correlated). To
reveal the simple geometry, we move to an <strong>Orthonormal
Frame</strong> (or "Risk Frame") where risk is uniform. We introduce the
square-root matrix (Vielbein) e ∞·µÉ·µó·µÉ·µ¢ such that Œ£ = e·µÄ e. In index
notation, this decomposition expresses the covariance matrix as: </p><p style="text-align:center;"><img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5CSigma_%7Bij%7D%20%3D%20%5Cdelta_%7B%5Chat%7Ba%7D%5Chat%7Bb%7D%7D%20e%5E%7B%5Chat%7Ba%7D%7D%7B%7D_i%20e%5E%7B%5Chat%7Bb%7D%7D%7B%7D_j."
alt="\Sigma_{ij} = \delta_{\hat{a}\hat{b}} e^{\hat{a}}{}_i e^{\hat{b}}{}_j."
title="\Sigma_{ij} = \delta_{\hat{a}\hat{b}} e^{\hat{a}}{}_i e^{\hat{b}}{}_j."
class="math display" /></p><p> In this flat frame, we define our primary
geometric objects:</p>
<ol type="1">
<li><p><strong>The Risk Portfolio (p)</strong>: The allocation vector
rescaled by risk. </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20p%5E%7B%5Chat%7Ba%7D%7D%20%3D%20e%5E%7B%5Chat%7Ba%7D%7D%7B%7D_i%20w%5Ei%20%5Cquad%20%28%5Capprox%20w%20%5Ccdot%20%5Csigma%29."
alt="p^{\hat{a}} = e^{\hat{a}}{}_i w^i \quad (\approx w \cdot \sigma)."
title="p^{\hat{a}} = e^{\hat{a}}{}_i w^i \quad (\approx w \cdot \sigma)."
class="math display" /></p><p> Its length is the total volatility: <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20%5C%7Cp%5C%7C%20%3D%20%5Csqrt%7B%5Cdelta_%7B%5Chat%7Ba%7D%5Chat%7Bb%7D%7D%20p%5E%7B%5Chat%7Ba%7D%7D%20p%5E%7B%5Chat%7Bb%7D%7D%7D%20%3D%20%5Csqrt%7Bw%5ET%20%5CSigma%20w%7D%20%3D%20%5Ctext%7BRisk%7D"
alt="\|p\| = \sqrt{\delta_{\hat{a}\hat{b}} p^{\hat{a}} p^{\hat{b}}} = \sqrt{w^T \Sigma w} = \text{Risk}"
title="\|p\| = \sqrt{\delta_{\hat{a}\hat{b}} p^{\hat{a}} p^{\hat{b}}} = \sqrt{w^T \Sigma w} = \text{Risk}"
class="math inline" />.</p></li>
<li><p><strong>The Skill Vector (s)</strong>: The alpha covector
rescaled by inverse risk (whitened).<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a> We obtain s by raising
the index of Œ± with the metric, then mapping to the frame: </p><p style="text-align:center;"><img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20s%5E%7B%5Chat%7Ba%7D%7D%20%3D%20e%5E%7B%5Chat%7Ba%7D%7D%7B%7D_k%20s%5Ek%20%3D%20e%5E%7B%5Chat%7Ba%7D%7D%7B%7D_k%20g%5E%7Bki%7D%20%5Calpha_i."
alt="s^{\hat{a}} = e^{\hat{a}}{}_k s^k = e^{\hat{a}}{}_k g^{ki} \alpha_i."
title="s^{\hat{a}} = e^{\hat{a}}{}_k s^k = e^{\hat{a}}{}_k g^{ki} \alpha_i."
class="math display" /></p><p> Its length is the <strong>Information
Coefficient (IC)</strong>: </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Ctext%7BIC%7D%20%3A%3D%20%5C%7Cs%5C%7C%20%3D%20%5Csqrt%7B%5Cdelta_%7B%5Chat%7Ba%7D%5Chat%7Bb%7D%7D%20s%5E%7B%5Chat%7Ba%7D%7D%20s%5E%7B%5Chat%7Bb%7D%7D%7D%20%3D%20%5Csqrt%7B%5Calpha_i%20g%5E%7Bij%7D%20%5Calpha_j%7D."
alt="\text{IC} := \|s\| = \sqrt{\delta_{\hat{a}\hat{b}} s^{\hat{a}} s^{\hat{b}}} = \sqrt{\alpha_i g^{ij} \alpha_j}."
title="\text{IC} := \|s\| = \sqrt{\delta_{\hat{a}\hat{b}} s^{\hat{a}} s^{\hat{b}}} = \sqrt{\alpha_i g^{ij} \alpha_j}."
class="math display" /></p></li>
</ol>
<h2 id="the-sharpe-ratio-and-transfer-coefficient">The Sharpe Ratio and
Transfer Coefficient</h2>
<p>In the Risk Frame, the metric is simply Œ¥‚Çï‚Çê‚Çú‚ÇêbÃÇ (Euclidean). The
Sharpe ratio is the projection of skill onto the implemented portfolio:
</p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Ceta%28w%29%20%3D%20%5Cfrac%7B%5Clangle%20s%2C%20p%20%5Crangle%7D%7B%5C%7Cp%5C%7C%7D."
alt="\eta(w) = \frac{\langle s, p \rangle}{\|p\|}."
title="\eta(w) = \frac{\langle s, p \rangle}{\|p\|}."
class="math display" /></p><p> This is maximized when p aligns with s. The
<strong>Transfer Coefficient (TC)</strong> is the cosine of the angle Œ∏
between them: </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Ctext%7BTC%7D%20%3A%3D%20%5Ccos%28%5Ctheta%29%20%3D%20%5Cfrac%7B%5Clangle%20s%2C%20p%20%5Crangle%7D%7B%5C%7Cs%5C%7C%5C%2C%5C%7Cp%5C%7C%7D."
alt="\text{TC} := \cos(\theta) = \frac{\langle s, p \rangle}{\|s\|\,\|p\|}."
title="\text{TC} := \cos(\theta) = \frac{\langle s, p \rangle}{\|s\|\,\|p\|}."
class="math display" /></p><p> Thus we recover the fundamental law: Sharpe = IC
√ó TC.</p>
<div class="remark">
<p><strong>Remark 1</strong> (Scale Invariance and Projective Space).
<em>The Sharpe ratio is invariant under the scaling of the portfolio
vector w ‚Üí Œª w for Œª &gt; 0 (leverage). This scale invariance
(homogeneity of degree 0) implies that the Sharpe ratio is not a
function on the vector space of portfolios, but rather on the
<strong>Projective Space</strong> (or the Sphere of Rays).
Geometrically, maximizing the Sharpe ratio is equivalent to finding a
point on the sphere SN‚Åª¬π that minimizes the geodesic distance to the
skill vector s.</em></p>
<div class="lemma">
<p><strong>Lemma 2</strong> (Orthogonal decomposition of Risk).
<em>Decompose the risk portfolio p into a component aligned with skill
and an orthogonal component: p = p‚à• + p‚ä•. Then: </p><p style="text-align:center;"><img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Cfrac%7B%5C%7Cp_%7B%5Cperp%7D%5C%7C%5E2%7D%7B%5C%7Cp%5C%7C%5E2%7D%20%3D%201%20-%20%5Ctext%7BTC%7D%5E2."
alt="\frac{\|p_{\perp}\|^2}{\|p\|^2} = 1 - \text{TC}^2."
title="\frac{\|p_{\perp}\|^2}{\|p\|^2} = 1 - \text{TC}^2."
class="math display" /></p><p> The orthogonal component p‚ä• corresponds to
variance consumed by positions that are uncorrelated with alpha. It is
"wasted risk".</em></p>
<h2 id="constrained-optimization-a-worked-example">Constrained
Optimization: A Worked Example</h2>
<p>In practice, portfolios face constraints (e.g., dollar-neutrality,
sector limits). Geometrically, constraints define a
<strong>submanifold</strong> ùíû ‚äÇ ‚ÑùN on which the portfolio must
live.</p>
<p><strong>Setup</strong>: Consider a two-asset world with holdings (w¬π,
w¬≤). Let Œ± = (Œ±‚ÇÅ, Œ±‚ÇÇ) be the alphas and Œ£ the covariance matrix. The
unconstrained optimal portfolio aligns with the skill vector s ∞·µÉ·µó·µÉ ‚àù
(Œ£‚Åª¬πŒ±)‚Å±.</p>
<p><strong>Constraint</strong>: Impose dollar-neutrality: w¬π + w¬≤ = 0.
This defines a one-dimensional submanifold (a line through the
origin).</p>
<p><strong>Geometric Effect</strong>: In the Risk Frame, the constraint
becomes a hyperplane (here, a line). The constrained optimum is the
<em>projection</em> of the skill vector s onto this hyperplane. Let
n ∞·µÉ·µó·µÉ be the unit normal to the constraint surface. The constrained
skill vector is: </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20s%5E%7B%5Chat%7Ba%7D%7D_%7B%5Ctext%7Bconstrained%7D%7D%20%3D%20s%5E%7B%5Chat%7Ba%7D%7D%20-%20%28s%20%5Ccdot%20n%29%20n%5E%7B%5Chat%7Ba%7D%7D."
alt="s^{\hat{a}}_{\text{constrained}} = s^{\hat{a}} - (s \cdot n) n^{\hat{a}}."
title="s^{\hat{a}}_{\text{constrained}} = s^{\hat{a}} - (s \cdot n) n^{\hat{a}}."
class="math display" /></p><p> The Transfer Coefficient under the constraint
becomes: </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Ctext%7BTC%7D_%7B%5Ctext%7Bconstrained%7D%7D%20%3D%20%5Cfrac%7B%5Clangle%20s_%7B%5Ctext%7Bconstrained%7D%7D%2C%20p%20%5Crangle%7D%7B%5C%7Cs_%7B%5Ctext%7Bconstrained%7D%7D%5C%7C%20%5C%7Cp%5C%7C%7D."
alt="\text{TC}_{\text{constrained}} = \frac{\langle s_{\text{constrained}}, p \rangle}{\|s_{\text{constrained}}\| \|p\|}."
title="\text{TC}_{\text{constrained}} = \frac{\langle s_{\text{constrained}}, p \rangle}{\|s_{\text{constrained}}\| \|p\|}."
class="math display" /></p><p> Since ‚Äñsc‚Çí‚Çô‚Çõ‚Çú·µ£‚Çê·µ¢‚Çô‚Çëd‚Äñ ‚â§ ‚Äñs‚Äñ, the constraint
<em>reduces achievable IC</em>. Meanwhile, the constrained portfolio can
now achieve TC = 1 (perfect alignment with sc‚Çí‚Çô‚Çõ‚Çú·µ£‚Çê·µ¢‚Çô‚Çëd), but with a
diminished target.</p>
<p><strong>Example</strong>: Let Œ£ = I (uncorrelated, unit variance),
and Œ± = (1, 0.5). The unconstrained skill vector is s = Œ± = (1, 0.5)
with <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20%5C%7Cs%5C%7C%20%3D%20%5Csqrt%7B1.25%7D%20%5Capprox%201.12"
alt="\|s\| = \sqrt{1.25} \approx 1.12"
title="\|s\| = \sqrt{1.25} \approx 1.12" class="math inline" />.</p>
<p>The dollar-neutral constraint w¬π + w¬≤ = 0 has normal <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20n%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%281%2C%201%29"
alt="n = \frac{1}{\sqrt{2}}(1, 1)" title="n = \frac{1}{\sqrt{2}}(1, 1)"
class="math inline" />. Projecting: </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20s_%7B%5Ctext%7Bconstrained%7D%7D%20%3D%20%281%2C%200.5%29%20-%20%5Cfrac%7B1.5%7D%7B2%7D%281%2C1%29%20%3D%20%280.25%2C%20-0.25%29."
alt="s_{\text{constrained}} = (1, 0.5) - \frac{1.5}{2}(1,1) = (0.25, -0.25)."
title="s_{\text{constrained}} = (1, 0.5) - \frac{1.5}{2}(1,1) = (0.25, -0.25)."
class="math display" /></p><p> The constrained IC is <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20%5C%7Cs_%7B%5Ctext%7Bconstrained%7D%7D%5C%7C%20%3D%20%5Csqrt%7B0.125%7D%20%5Capprox%200.35"
alt="\|s_{\text{constrained}}\| = \sqrt{0.125} \approx 0.35"
title="\|s_{\text{constrained}}\| = \sqrt{0.125} \approx 0.35"
class="math inline" />, a 69% reduction. The optimal constrained
portfolio is w ‚àù (1, -1), which aligns perfectly with sc‚Çí‚Çô‚Çõ‚Çú·µ£‚Çê·µ¢‚Çô‚Çëd.</p>
<div class="remark">
<p><strong>Remark 3</strong> (Multiple Constraints). <em>With k linear
constraints, the feasible set is an (N-k)-dimensional affine subspace.
The projection becomes orthogonal projection onto this subspace.
Non-linear constraints (e.g., box constraints |w‚Å±| ‚â§ 1) yield curved
submanifolds, and the projection becomes a geodesic projection‚Äîgenerally
requiring numerical methods.</em></p>
<h1 id="part-ii-the-geometry-of-statistical-inference">Part II: The
Geometry of Statistical Inference</h1>
<p>We now shift focus from the geometry of assets to the geometry of
probability distributions.</p>
<h2 id="the-manifold-of-moments">The Manifold of Moments</h2>
<p>Let our statistical manifold ùíµ be coordinatized by the raw moments
z^Œº (e.g., sample means, sample second moments). The Central Limit
Theorem (CLT) governs the fluctuations of these moments. It states that
the fluctuations Œ¥ z^Œº = zÃÇ^Œº - z^Œº live in the tangent space Tzùíµ and are
distributed as a multivariate Gaussian with covariance tensor ŒìŒºŒΩ: </p><p style="text-align:center;"><img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Csqrt%7BT%7D%20%5Cdelta%20z%20%5Csim%20%5Cmathcal%7BN%7D%280%2C%20%5CGamma%29."
alt="\sqrt{T} \delta z \sim \mathcal{N}(0, \Gamma)."
title="\sqrt{T} \delta z \sim \mathcal{N}(0, \Gamma)."
class="math display" /></p><p> ŒìŒºŒΩ = ùîº[Œ¥ z^Œº Œ¥ z^ŒΩ] is a (2, 0) tensor field on
ùíµ.</p>
<h2 id="the-delta-method-as-a-pushforward">The Delta Method as a
Pushforward</h2>
<p>We are interested in derived parameters Œ∏·µÖ (e.g., variances, Sharpe
ratios) which are functions of z. Let œÜ: ùíµ ‚Üí ùí´ be the map from moments
to parameters. The linear map deriving the fluctuations in parameters (Œ¥
Œ∏) from fluctuations in moments (Œ¥ z) is the <strong>Pushforward
Map</strong> (or Jacobian) E: Tzùíµ ‚Üí T_Œ∏ùí´. Its components are: </p><p style="text-align:center;"><img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20E%5E%5Calpha%7B%7D_%5Cmu%20%3D%20%5Cfrac%7B%5Cpartial%20%5Ctheta%5E%5Calpha%7D%7B%5Cpartial%20z%5E%5Cmu%7D."
alt="E^\alpha{}_\mu = \frac{\partial \theta^\alpha}{\partial z^\mu}."
title="E^\alpha{}_\mu = \frac{\partial \theta^\alpha}{\partial z^\mu}."
class="math display" /></p><p> The covariance tensor on the parameter manifold
ùí´ is the pushforward of Œì: </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5COmega%5E%7B%5Calpha%5Cbeta%7D%20%3D%20E%5E%5Calpha%7B%7D_%5Cmu%20E%5E%5Cbeta%7B%7D_%5Cnu%20%5CGamma%5E%7B%5Cmu%5Cnu%7D."
alt="\Omega^{\alpha\beta} = E^\alpha{}_\mu E^\beta{}_\nu \Gamma^{\mu\nu}."
title="\Omega^{\alpha\beta} = E^\alpha{}_\mu E^\beta{}_\nu \Gamma^{\mu\nu}."
class="math display" /></p><p> In compact notation: Œ© = œÜ_* Œì. This corresponds
to the standard transformation rule for (2,0) tensors under a change of
coordinates.</p>
<h1 id="part-iii-application-to-sharpe-difference">Part III: Application
to Sharpe Difference</h1>
<p>We apply this machinery to find the variance of the difference
between two Sharpe ratios, Œî Œ∑ = Œ∑‚ÇÅ - Œ∑‚ÇÇ.</p>
<h2 id="coordinates-and-tensor-components">Coordinates and Tensor
Components</h2>
<ol type="1">
<li><p><strong>Raw Coordinates (z^Œº)</strong>: (Œº‚ÇÅ, Œº‚ÇÇ, q‚ÇÅ, q‚ÇÇ) where q·µ¢
= ùîº[r·µ¢¬≤].</p></li>
<li><p><strong>Parameter Coordinates (Œ∏·µÖ)</strong>: (Œº‚ÇÅ, Œº‚ÇÇ, v‚ÇÅ, v‚ÇÇ)
where v·µ¢ = œÉ·µ¢¬≤ = q·µ¢ - Œº·µ¢¬≤.</p></li>
</ol>
<p>The Jacobian E·µÖ_Œº transforming Œ¥ z to Œ¥ Œ∏ is: </p><p style="text-align:center;"><img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20E%20%3D%20%5Cbegin%7Bpmatrix%7D%0A%20%20%20%20%20%20%20%201%20%26%200%20%26%200%20%26%200%20%5C%5C%0A%20%20%20%20%20%20%20%200%20%26%201%20%26%200%20%26%200%20%5C%5C%0A%20%20%20%20%20%20%20%20-2%5Cmu_1%20%26%200%20%26%201%20%26%200%20%5C%5C%0A%20%20%20%20%20%20%20%200%20%26%20-2%5Cmu_2%20%26%200%20%26%201%0A%20%20%20%20%5Cend%7Bpmatrix%7D."
alt="E = \begin{pmatrix}
        1 &amp; 0 &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; 0 &amp; 0 \\
        -2\mu_1 &amp; 0 &amp; 1 &amp; 0 \\
        0 &amp; -2\mu_2 &amp; 0 &amp; 1
    \end{pmatrix}." title="E = \begin{pmatrix}
        1 &amp; 0 &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; 0 &amp; 0 \\
        -2\mu_1 &amp; 0 &amp; 1 &amp; 0 \\
        0 &amp; -2\mu_2 &amp; 0 &amp; 1
    \end{pmatrix}." class="math display" /></p><p> The Covariance Tensor Œ© in
parameter space, assuming Gaussian returns (Basu's theorem implies
independence of sample mean and variance), is block diagonal: </p><p style="text-align:center;"><img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5COmega%20%3D%20%5Cbegin%7Bpmatrix%7D%0A%20%20%20%20%20%20%20%20%20v_1%20%26%20%5Crho%5Csqrt%7Bv_1v_2%7D%20%26%200%20%26%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%5Crho%5Csqrt%7Bv_1v_2%7D%20%26%20v_2%20%26%200%20%26%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%200%20%26%200%20%26%202v_1%5E2%20%26%202%5Crho%5E2%20v_1%20v_2%20%5C%5C%0A%20%20%20%20%20%20%20%20%200%20%26%200%20%26%202%5Crho%5E2%20v_1%20v_2%20%26%202v_2%5E2%0A%20%20%20%20%5Cend%7Bpmatrix%7D."
alt="\Omega = \begin{pmatrix}
         v_1 &amp; \rho\sqrt{v_1v_2} &amp; 0 &amp; 0 \\
         \rho\sqrt{v_1v_2} &amp; v_2 &amp; 0 &amp; 0 \\
         0 &amp; 0 &amp; 2v_1^2 &amp; 2\rho^2 v_1 v_2 \\
         0 &amp; 0 &amp; 2\rho^2 v_1 v_2 &amp; 2v_2^2
    \end{pmatrix}." title="\Omega = \begin{pmatrix}
         v_1 &amp; \rho\sqrt{v_1v_2} &amp; 0 &amp; 0 \\
         \rho\sqrt{v_1v_2} &amp; v_2 &amp; 0 &amp; 0 \\
         0 &amp; 0 &amp; 2v_1^2 &amp; 2\rho^2 v_1 v_2 \\
         0 &amp; 0 &amp; 2\rho^2 v_1 v_2 &amp; 2v_2^2
    \end{pmatrix}." class="math display" /></p>
<h2 id="the-gradient-covector">The Gradient Covector</h2>
<p>Our target function is the scalar field <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20f%28%5Ctheta%29%20%3D%20%5Ceta_1%20-%20%5Ceta_2%20%3D%20%5Cfrac%7B%5Cmu_1%7D%7B%5Csqrt%7Bv_1%7D%7D%20-%20%5Cfrac%7B%5Cmu_2%7D%7B%5Csqrt%7Bv_2%7D%7D"
alt="f(\theta) = \eta_1 - \eta_2 = \frac{\mu_1}{\sqrt{v_1}} - \frac{\mu_2}{\sqrt{v_2}}"
title="f(\theta) = \eta_1 - \eta_2 = \frac{\mu_1}{\sqrt{v_1}} - \frac{\mu_2}{\sqrt{v_2}}"
class="math inline" />. Its differential df is a covector with
components <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20g_%5Calpha%20%3D%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%5E%5Calpha%7D"
alt="g_\alpha = \frac{\partial f}{\partial \theta^\alpha}"
title="g_\alpha = \frac{\partial f}{\partial \theta^\alpha}"
class="math inline" />: </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20g_%5Calpha%20%3D%20%5Cbegin%7Bpmatrix%7D%20v_1%5E%7B-1%2F2%7D%20%5C%5C%20-v_2%5E%7B-1%2F2%7D%20%5C%5C%20-%5Cfrac%7B1%7D%7B2%7D%5Cmu_1%20v_1%5E%7B-3%2F2%7D%20%5C%5C%20%5Cfrac%7B1%7D%7B2%7D%5Cmu_2%20v_2%5E%7B-3%2F2%7D%20%5Cend%7Bpmatrix%7D."
alt="g_\alpha = \begin{pmatrix} v_1^{-1/2} \\ -v_2^{-1/2} \\ -\frac{1}{2}\mu_1 v_1^{-3/2} \\ \frac{1}{2}\mu_2 v_2^{-3/2} \end{pmatrix}."
title="g_\alpha = \begin{pmatrix} v_1^{-1/2} \\ -v_2^{-1/2} \\ -\frac{1}{2}\mu_1 v_1^{-3/2} \\ \frac{1}{2}\mu_2 v_2^{-3/2} \end{pmatrix}."
class="math display" /></p>
<p><strong>Geometric Interpretation (Foliation)</strong>: The function
f(Œ∏) defines a <em>foliation</em> of the statistical manifold by level
sets (hypersurfaces of constant Sharpe difference). The gradient
covector g is the <strong>normal vector</strong> to these leaves. The
asymptotic variance is simply the "squared length" of this normal
vector, as measured by the covariance metric Œ©. It represents the cost
of moving orthogonal to the foliation‚Äîi.e., the uncertainty in the
direction that actually changes the Sharpe ratio.</p>
<h2 id="metric-contraction-the-result">Metric Contraction (The
Result)</h2>
<p>The asymptotic variance is the scalar obtained by contracting the
gradient covector with the covariance tensor: </p><p style="text-align:center;"><img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Cmathrm%7BVar%7D%28%5CDelta%20%5Ceta%29%20%5Capprox%20%5Cfrac%7B1%7D%7BT%7D%20%5COmega%5E%7B%5Calpha%5Cbeta%7D%20%28df%29_%5Calpha%20%28df%29_%5Cbeta."
alt="\mathrm{Var}(\Delta \eta) \approx \frac{1}{T} \Omega^{\alpha\beta} (df)_\alpha (df)_\beta."
title="\mathrm{Var}(\Delta \eta) \approx \frac{1}{T} \Omega^{\alpha\beta} (df)_\alpha (df)_\beta."
class="math display" /></p><p> <strong>Geometric Intuition:</strong> Just as
the length of a vector v is ‚Äñv‚Äñ¬≤ = g·µ¢‚±º v‚Å± v ≤, the "uncertainty" of a
scalar function f is the squared length of its gradient df measured by
the covariance metric Œ©. This quantity is a scalar invariant: it does
not depend on which coordinate system (e.g., central moments vs.¬†raw
moments) we use to perform the calculation. Performing the matrix
multiplication (see <code>derivations.ipynb</code> for symbolic
verification): </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Cbegin%7Baligned%7D%0A%20%20%20%20%5COmega%5E%7B%5Calpha%5Cbeta%7D%20%28df%29_%5Calpha%20%28df%29_%5Cbeta%20%26%3D%20%5Cunderbrace%7Bv_1%28df%29_1%5E2%20%2B%20v_2%28df%29_2%5E2%20%2B%202%5Crho%5Csqrt%7Bv_1v_2%7D%28df%29_1%28df%29_2%7D_%7B%5Ctext%7BMean%20terms%7D%7D%20%5C%5C%0A%20%20%20%20%26%2B%20%5Cunderbrace%7B2v_1%5E2%28df%29_3%5E2%20%2B%202v_2%5E2%28df%29_4%5E2%20%2B%202%282%5Crho%5E2%20v_1%20v_2%29%28df%29_3%28df%29_4%7D_%7B%5Ctext%7BVariance%20terms%7D%7D.%0A%5Cend%7Baligned%7D"
alt="\begin{aligned}
    \Omega^{\alpha\beta} (df)_\alpha (df)_\beta &amp;= \underbrace{v_1(df)_1^2 + v_2(df)_2^2 + 2\rho\sqrt{v_1v_2}(df)_1(df)_2}_{\text{Mean terms}} \\
    &amp;+ \underbrace{2v_1^2(df)_3^2 + 2v_2^2(df)_4^2 + 2(2\rho^2 v_1 v_2)(df)_3(df)_4}_{\text{Variance terms}}.
\end{aligned}" title="\begin{aligned}
    \Omega^{\alpha\beta} (df)_\alpha (df)_\beta &amp;= \underbrace{v_1(df)_1^2 + v_2(df)_2^2 + 2\rho\sqrt{v_1v_2}(df)_1(df)_2}_{\text{Mean terms}} \\
    &amp;+ \underbrace{2v_1^2(df)_3^2 + 2v_2^2(df)_4^2 + 2(2\rho^2 v_1 v_2)(df)_3(df)_4}_{\text{Variance terms}}.
\end{aligned}" class="math display" /></p><p> Substituting the components
yields: </p><p style="text-align:center;"><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20%5Cmathrm%7BVar%7D%28%5CDelta%20%5Ceta%29%20%5Capprox%20%5Cfrac%7B1%7D%7BT%7D%20%5Cleft%5B%202%281-%5Crho%29%20%2B%20%5Cfrac%7B1%7D%7B2%7D%28%5Ceta_1%5E2%20%2B%20%5Ceta_2%5E2%29%20-%20%5Crho%5E2%20%5Ceta_1%5Ceta_2%20%5Cright%5D."
alt="\mathrm{Var}(\Delta \eta) \approx \frac{1}{T} \left[ 2(1-\rho) + \frac{1}{2}(\eta_1^2 + \eta_2^2) - \rho^2 \eta_1\eta_2 \right]."
title="\mathrm{Var}(\Delta \eta) \approx \frac{1}{T} \left[ 2(1-\rho) + \frac{1}{2}(\eta_1^2 + \eta_2^2) - \rho^2 \eta_1\eta_2 \right]."
class="math display" /></p><p> This recovers the Jobson‚ÄìKorkie result as a
clean geometric contraction.<a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
</div>
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p><strong>Thermodynamic Analogy</strong>: A useful
physical parallel is Thermodynamics. Allocations w are
<strong>Extensive</strong> variables (they scale with system size, like
Volume V). Alphas Œ± are <strong>Intensive</strong> variables (they do
not scale, like Pressure P). Their pairing PnL (like Energy) is
naturally a scalar. The "density" interpretation arises because we often
normalize w (setting w·µ¢ = 1) to remove the extensive scale freedom.<a
href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>Computations are often simpler in this orthonormal
frame. Here aÃÇ,bÃÇ are frame indices (flat/hatted) and i,j are coordinate
indices (curved). The Sharpe ratio becomes the standard Euclidean dot
product projection.<a href="#fnref2" class="footnote-back"
role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>If returns are not Gaussian, the covariance tensor Œ©
acquires off-diagonal blocks relating means and variances (skewness).
Geometrically, this introduces "torsion" to the statistical manifold,
twisting the independence of the first and second moments.<a
href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section>
</body>
</html>
